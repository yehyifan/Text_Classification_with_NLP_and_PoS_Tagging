{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yehyifan/Text_Classification_with_NLP_and_PoS_Tagging/blob/main/Text_Classification_with_NLP_and_PoS_Tagging_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification with NLP & PoS Tagging  \n",
        "Built a natural language processing pipeline to convert English sentences into musical \"emoji symphonies\" by mapping each word to an emoji based on its part of speech. Used **NLTK** for tokenization, stopword removal, PoS tagging, and **WordNet** for lexical exploration. Designed a system to visualize syntactic patterns creatively through emoji encoding, enhancing understanding of sentence structure and linguistic roles.\n"
      ],
      "metadata": {
        "id": "oE_5M19vMYdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Import and Data Loading"
      ],
      "metadata": {
        "id": "N1oxWQ5oMt4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wWd4ZOuMX6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc861977-478a-442f-e273-f81a863fd4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from collections import defaultdict, Counter\n",
        "from ast import literal_eval\n",
        "import pickle\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tag import pos_tag\n",
        "stops = set(stopwords.words('english'))\n",
        "from tqdm import tqdm\n",
        "from urllib import request\n",
        "module_url = [f\"https://drive.google.com/uc?export=view&id=179zVybyc3cCCS5IG9F-STx2Z8pzrGQZ6\",\n",
        "              f\"https://drive.google.com/uc?export=view&id=1MH56eN-QXyqny35VzbddI7MxA-oGO0IT\"]\n",
        "name = ['big.txt', 'sorted_symphs.pkl']\n",
        "for i in range(len(name)):\n",
        "    with request.urlopen(module_url[i]) as f, open(name[i],'w') as outf:\n",
        "        a = f.read()\n",
        "        outf.write(a.decode('ISO-8859-1'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rhythm(line):\n",
        "    \"\"\"\n",
        "    Convert a line of text into a sequence of emojis based on POS tags.\n",
        "\n",
        "    Args:\n",
        "        line: A string of text to process.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples, each containing:\n",
        "        - the original sentence\n",
        "        - the POS-tagged sentence\n",
        "        - the emoji sequence representing the sentence's rhythm\n",
        "    \"\"\"\n",
        "    # Apply sent_tokenize to the line\n",
        "    sentences = sent_tokenize(line)\n",
        "    results = []\n",
        "\n",
        "    for original_sentence in sentences:\n",
        "        # Tokenize and POS tag the sentence\n",
        "        tokens = word_tokenize(original_sentence)\n",
        "        pos_tagged_sentence = pos_tag(tokens)\n",
        "\n",
        "        # Create emoji sequence based on POS tags\n",
        "        emoji_sequence = ''\n",
        "        for word, tag in pos_tagged_sentence:\n",
        "            if word.lower() in stops:\n",
        "                emoji_sequence += 'ðŸŽµ'\n",
        "            elif tag.startswith('N'):\n",
        "                emoji_sequence += 'ðŸŽ¸'\n",
        "            elif tag.startswith('V'):\n",
        "                emoji_sequence += 'ðŸŽ¹'\n",
        "            elif tag.startswith('J'):\n",
        "                emoji_sequence += 'ðŸŽ·'\n",
        "            else:\n",
        "                emoji_sequence += 'ðŸŽ¤'\n",
        "\n",
        "        # Add to results\n",
        "        results.append((original_sentence, pos_tagged_sentence, emoji_sequence))\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_symphonies(corpus_path):\n",
        "    \"\"\"\n",
        "    Build and sort emoji-based rhythm patterns from a text corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus_path: Path to the text corpus file.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples:\n",
        "        - emoji sequence (str)\n",
        "        - list of unique POS-tagged sentences matching that sequence, sorted by group size\n",
        "    \"\"\"\n",
        "    # Dictionary to store emoji sequences and their corresponding tagged sentences\n",
        "    symphony_dict = defaultdict(list)\n",
        "\n",
        "    # Process the corpus line by line\n",
        "    with open(corpus_path, 'r', encoding='ISO-8859-1') as file:\n",
        "        for line in file:\n",
        "            if not line.isspace():\n",
        "                rhythm_results = get_rhythm(line)\n",
        "\n",
        "                for _, pos_tagged_sentence, emoji_sequence in rhythm_results:\n",
        "                    # Check if this exact POS-tagged sentence is already in the list\n",
        "                    if pos_tagged_sentence not in symphony_dict[emoji_sequence]:\n",
        "                        symphony_dict[emoji_sequence].append(pos_tagged_sentence)\n",
        "\n",
        "    # Sort by the number of sentences each emoji sequence produces (largest first)\n",
        "    sorted_symphs = sorted(symphony_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "    return sorted_symphs\n",
        "\n",
        "sorted_symphs = get_symphonies('big.txt')"
      ],
      "metadata": {
        "id": "F8bqUZGZYKTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with one entry\n",
        "a = sorted_symphs[1176]\n",
        "el = a[0]\n",
        "list_of_postagged = a[1]\n",
        "print(el)\n",
        "for pos_tagged in list_of_postagged:\n",
        "  print(' '.join([w for w,p in pos_tagged]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUW_ONWbJPRm",
        "outputId": "7feab7fc-2f64-4000-95bd-35920d359e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¤ðŸŽ¸ðŸŽ¹ðŸŽµðŸŽ¤ðŸŽ¤\n",
            "`` God help me ! ''\n",
            "`` Nobody wants me ! ''\n",
            "`` Devil take them ! ''\n",
            "`` Devil take it ! ''\n",
            "`` Natasha told me . ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sentences_with_most_specific_nouns(sorted_symphs):\n",
        "    \"\"\"\n",
        "    Identify the most specific noun in each POS pattern group.\n",
        "\n",
        "    Performance notes:\n",
        "    - Optimized to run in under 5 seconds on a Google Colab notebook\n",
        "    - First run after session restart may take longer due to WordNet initialization\n",
        "    - Uses intelligent sampling and caching for efficiency\n",
        "\n",
        "    Args:\n",
        "        sorted_symphs: List of (pattern, list of POS-tagged sentences)\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with:\n",
        "            - 'pattern': POS tag pattern\n",
        "            - 'pos_sentence': sentence with the most specific noun\n",
        "            - 'noun': most specific noun\n",
        "            - 'path_length': WordNet path depth\n",
        "            - 'synsets': associated WordNet synsets\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    noun_depth_cache = {}\n",
        "\n",
        "    for emoji_pattern, sentences in tqdm(sorted_symphs, desc=\"Finding specific nouns\", unit=\"symphony\"):\n",
        "    # for emoji_pattern, sentences in tqdm(sorted_symphs):\n",
        "        the_most_specific_noun_in_that_sentence = None\n",
        "        pos_tagged_sentence = None\n",
        "        max_distance_from_synsets_for_noun_to_wn_root = 0\n",
        "        list_of_synsets_mapping_to_noun = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word, pos in sentence:\n",
        "                if not pos.startswith('NN'):\n",
        "                    continue\n",
        "\n",
        "                noun_key = word.lower()\n",
        "\n",
        "                if noun_key in noun_depth_cache:\n",
        "                    cached = noun_depth_cache[noun_key]\n",
        "                    current_depth = cached['depth']\n",
        "                    synset_names = cached['synsets']\n",
        "                else:\n",
        "                    synsets = wn.synsets(word, pos='n') or wn.synsets(noun_key, pos='n')\n",
        "                    if not synsets:\n",
        "                        continue\n",
        "                    current_depth = max(len(path) for syn in synsets for path in syn.hypernym_paths())\n",
        "                    synset_names = [syn.name() for syn in synsets]\n",
        "                    noun_depth_cache[noun_key] = {\n",
        "                        'depth': current_depth,\n",
        "                        'synsets': synset_names\n",
        "                    }\n",
        "\n",
        "                if current_depth > max_distance_from_synsets_for_noun_to_wn_root:\n",
        "                    the_most_specific_noun_in_that_sentence = word\n",
        "                    pos_tagged_sentence = sentence\n",
        "                    max_distance_from_synsets_for_noun_to_wn_root = current_depth\n",
        "                    list_of_synsets_mapping_to_noun = synset_names\n",
        "\n",
        "        if the_most_specific_noun_in_that_sentence:\n",
        "            results.append({\n",
        "                'pattern': emoji_pattern,\n",
        "                'pos_sentence': pos_tagged_sentence,\n",
        "                'noun': the_most_specific_noun_in_that_sentence,\n",
        "                'path_length': max_distance_from_synsets_for_noun_to_wn_root,\n",
        "                'synsets': list_of_synsets_mapping_to_noun\n",
        "            })\n",
        "\n",
        "    # Sort results by descending specificity\n",
        "    results.sort(key=lambda x: x['path_length'], reverse=True)\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "# if you could not solve the first part, please use this loading step below.\n",
        "# Otherwise, use your own sorted_symphs from previous part of the question.\n",
        "# sorted_symphs = pickle.load(open('sorted_symphs.pkl', 'rb'))\n",
        "specific_sentences = find_sentences_with_most_specific_nouns(sorted_symphs)\n",
        "\n",
        "print(len(specific_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JM7o53T9Pw6",
        "outputId": "9c6b1b8d-33fa-4c8b-f291-20444fef4dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding specific nouns: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84789/84789 [00:02<00:00, 29867.22symphony/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "print(\"\\nMost specific sentences by pattern:\")\n",
        "for result in specific_sentences[:3]:  # Show top 3\n",
        "    print(f\"\\nPattern: {result['pattern']}\")\n",
        "    print(f\"POS tagged sentence: {result['pos_sentence']}\")\n",
        "    print(f\"Most specific noun: {result['noun']}\")\n",
        "    print(f\"Path length to root: {result['path_length']}\")\n",
        "    print(f\"Synsets: {result['synsets']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBBb4CivoDdj",
        "outputId": "2cc58622-1ad3-4bcd-e5ec-2a7c2d730cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most specific sentences by pattern:\n",
            "\n",
            "Pattern: ðŸŽ¸ðŸŽµðŸŽ¸ðŸŽ¤\n",
            "POS tagged sentence: [('Jersey', 'NNP'), ('into', 'IN'), ('Pennsylvania', 'NNP'), ('.', '.')]\n",
            "Most specific noun: Jersey\n",
            "Path length to root: 19\n",
            "Synsets: ['new_jersey.n.01', 'jersey.n.02', 'jersey.n.03', 'jersey.n.04', 'jersey.n.05']\n",
            "\n",
            "Pattern: ðŸŽ¸ðŸŽµðŸŽ¸ðŸŽ¸ðŸŽ¤\n",
            "POS tagged sentence: [('Paterson', 'NNP'), ('of', 'IN'), ('New', 'NNP'), ('Jersey', 'NNP'), ('flatly', 'RB')]\n",
            "Most specific noun: Jersey\n",
            "Path length to root: 19\n",
            "Synsets: ['new_jersey.n.01', 'jersey.n.02', 'jersey.n.03', 'jersey.n.04', 'jersey.n.05']\n",
            "\n",
            "Pattern: ðŸŽ¸ðŸŽ¸ðŸŽµ\n",
            "POS tagged sentence: [('New', 'NNP'), ('Jersey', 'NNP'), ('had', 'VBD')]\n",
            "Most specific noun: Jersey\n",
            "Path length to root: 19\n",
            "Synsets: ['new_jersey.n.01', 'jersey.n.02', 'jersey.n.03', 'jersey.n.04', 'jersey.n.05']\n"
          ]
        }
      ]
    }
  ]
}